{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avasiliki/MyProjects/blob/main/search_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv9ujrKit831"
      },
      "source": [
        "# Εξάγωγή ειδήσεων από RSS feeds 5 ειδησεογραφικών καναλιών: enikos, naftemporiki, in,protothema και tanea.Στη συνέχεια αποθήκευση αυτών στο dataframe df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4suHyNks7U7"
      },
      "outputs": [],
      "source": [
        "!pip install feedparser\n",
        "from dateutil import parser\n",
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import pytz\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIIEWXk4qkW0"
      },
      "outputs": [],
      "source": [
        "timeZ_At = pytz.timezone('Europe/Athens') #παίρνουμε τη ζώνη ώρας για την Ελλάδα\n",
        "# Εξαγωγή ειδήσεων από το ειδησεογραφικό κανάλι enikos.gr\n",
        "def read_enikos_rss():\n",
        "\n",
        " #URL προς απόξεση\n",
        " url_to_scrape=\"https://www.enikos.gr/feed/\"\n",
        " NewsFeed = feedparser.parse(url_to_scrape)\n",
        " \n",
        " news_items=[]  \n",
        " for entry in NewsFeed.entries:\n",
        "    news_item = {}\n",
        "    news_item['title'] = entry.title\n",
        "    news_item['category'] = entry.category\n",
        "    news_item['description'] = entry.description\n",
        "    news_item['link'] = entry.link\n",
        "    datetime_rss = parser.parse(entry.published)\n",
        "    datetime_timestamp = float(datetime_rss.astimezone(timeZ_At).strftime(\"%s\"))    #μετατροπή ώρας στην τοπική\n",
        "    local_datetime_converted = datetime.datetime.fromtimestamp(datetime_timestamp)  #μετατροπή ημερομηνίας/ώρας σε μορφή κοινή για όλα τα ειδησεογραφικά κανάλια\n",
        "    news_item['pubDate'] = local_datetime_converted \n",
        "\n",
        "    #  Ανάγνωση του περιεχομένου των ειδήσεων \n",
        "    news = requests.get(entry.link)\n",
        "    news_content = news.content  \n",
        "    soup_news = BeautifulSoup(news_content, 'html.parser')\n",
        "    for script in soup_news(['script']):\n",
        "         script.decompose()  # αφαίρεση ετικετών script και του περιεχομένου τους\n",
        "        \n",
        "    b = soup_news.find_all('div', class_='articletext') \n",
        "    try:\n",
        "       x=b[0].get_text()\n",
        "    except IndexError:\n",
        "       x = \"\"\n",
        "\n",
        "    # Αφαίρεση κενών γραμμών από το περιεχόμενο\n",
        "    lines = x.split(\"\\n\")\n",
        "    non_empty_lines = [line for line in lines if line.strip() != \"\"]\n",
        "    text_without_empty_lines = \"\"\n",
        "    for line in non_empty_lines:\n",
        "      text_without_empty_lines += line + \" \"\n",
        "            \n",
        "    news_item['content']= re.sub(r\"http\\S+\", \"\", text_without_empty_lines)\n",
        "    \n",
        "    news_items.append(news_item)\n",
        " return news_items\n",
        "\n",
        "# Εξαγωγή ειδήσεων από το ειδησεογραφικό κανάλι naftemporiki.gr\n",
        "def read_naftemporiki_rss():\n",
        "\n",
        " #URL προς απόξεση\n",
        " url_to_scrape=\"https://www.naftemporiki.gr/rssFeed\"\n",
        " NewsFeed = feedparser.parse(url_to_scrape)\n",
        "\n",
        " news_items=[]  \n",
        " for entry in NewsFeed.entries:\n",
        "    news_item = {}\n",
        "    news_item['title'] = entry.title\n",
        "    news_item['category'] = ' '\n",
        "    news_item['description'] = entry.description\n",
        "    news_item['link'] = entry.link\n",
        "    datetime_rss = parser.parse(entry.published)\n",
        "    datetime_timestamp = float(datetime_rss.strftime(\"%s\"))\n",
        "    local_datetime_converted = datetime.datetime.fromtimestamp(datetime_timestamp) #μετατροπή ημερομηνίας/ώρας σε μορφή κοινή για όλα τα ειδησεογραφικά κανάλια\n",
        "    news_item['pubDate'] = local_datetime_converted \n",
        "\n",
        "    #  Ανάγνωση του περιεχομένου των ειδήσεων \n",
        "    news = requests.get(entry.link)\n",
        "    news_content = news.content   \n",
        "    soup_news = BeautifulSoup(news_content, 'html.parser')\n",
        "    b = soup_news.find_all('span', id='spBody')\n",
        "    try:\n",
        "       x = b[0].find_all(['p','li'])\n",
        "    except IndexError:\n",
        "       x = []\n",
        "\n",
        "    # Ένωση των παραγράφων\n",
        "    list_paragraphs = []\n",
        "    final_article=\" \"\n",
        "    for p in range(len(x)):\n",
        "        paragraph = x[p].get_text() + \" \"\n",
        "        list_paragraphs.append(paragraph) \n",
        "    final_article = \" \".join(list_paragraphs)\n",
        "    news_item['content']=final_article \n",
        "    \n",
        "    news_items.append(news_item)\n",
        " return news_items\n",
        "\n",
        "# Εξαγωγή ειδήσεων από το ειδησεογραφικό κανάλι in.gr \n",
        "def read_in_rss():\n",
        "\n",
        " #URL προς απόξεση\n",
        " url_to_scrape=\"https://www.in.gr/feed/\"\n",
        " NewsFeed = feedparser.parse(url_to_scrape)\n",
        "\n",
        " news_items=[]  \n",
        " for entry in NewsFeed.entries:\n",
        "    news_item = {}\n",
        "    news_item['title'] = entry.title\n",
        "    news_item['category'] = entry.category\n",
        "    soup_description= BeautifulSoup(entry.description, 'html.parser')\n",
        "    news_item['description'] = soup_description.get_text()  \n",
        "    news_item['link'] = entry.link\n",
        "    datetime_rss = parser.parse(entry.published)\n",
        "    datetime_timestamp = float(datetime_rss.astimezone(timeZ_At).strftime(\"%s\"))   #μετατροπή ώρας στην τοπική\n",
        "    local_datetime_converted = datetime.datetime.fromtimestamp(datetime_timestamp) #μετατροπή ημερομηνίας/ώρας σε μορφή κοινή για όλα τα ειδησεογραφικά κανάλια\n",
        "    news_item['pubDate'] = local_datetime_converted \n",
        "\n",
        "    #  Ανάγνωση του περιεχομένου των ειδήσεων \n",
        "    news = requests.get(entry.link)\n",
        "    \n",
        "    news_content = news.content   \n",
        "    soup_news = BeautifulSoup(news_content, 'html.parser')\n",
        "    #ανάγνωση ειδήσεων από το h2 tag στο πάνω  μέρος του άρθρου\n",
        "    b1=soup_news.find_all('h2', class_='description' )\n",
        "    #ανάγνωση του υπόλοιπου άρθρου ειδήσεων\n",
        "    b = soup_news.find_all('div', itemprop='articleBody')\n",
        "\n",
        "    # το άρθρο δεν έχει σταθερή μορφή    \n",
        "    try:\n",
        "      if len(b)==2:  # 1η μορφή\n",
        "       x = b[1].find_all([\"p\",\"h1\",\"h2\",\"h3\",\"h4\"])\n",
        "      else: # 2η μορφή\n",
        "       x = b[0].find_all([\"p\",\"h1\",\"h2\",\"h3\",\"h4\"])\n",
        "    except IndexError:\n",
        "      x = []\n",
        "     \n",
        "           \n",
        "    # Ένωση των παραγράφων\n",
        "    list_paragraphs = []\n",
        "    final_article=\"  \"\n",
        "    try:\n",
        "      paragraph=b1[0].get_text() #κείμενο απο το h2 tag στο πάνω  μέρος του άρθρου     \n",
        "    except:\n",
        "      paragraph=\"\"     \n",
        "    list_paragraphs.append(paragraph) \n",
        "\n",
        "    if len(b)==2: # 1η μορφή\n",
        "      for p in range(len(x)):\n",
        "        paragraph = x[p].get_text() + \" \"\n",
        "        list_paragraphs.append(paragraph) \n",
        "    else:  #2η μορφή\n",
        "     try:\n",
        "       paragraph = x[0].get_text() + \" \" \n",
        "     except:\n",
        "       paragraph=\"\"\n",
        "     list_paragraphs.append(paragraph) \n",
        "    final_article = \" \".join(list_paragraphs) #τελικό περιεχόμενο του άρθρου\n",
        "    news_item['content']=final_article \n",
        "        \n",
        "    news_items.append(news_item)\n",
        " return news_items\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Εξαγωγή ειδήσεων από το ειδησεογραφικό κανάλι protothema.gr \n",
        "def read_protothema_rss():\n",
        "\n",
        " #URL προς απόξεση\n",
        " url_to_scrape=\"https://www.protothema.gr/rss\"\n",
        " NewsFeed = feedparser.parse(url_to_scrape)\n",
        " \n",
        "\n",
        " news_items=[]  \n",
        " for entry in NewsFeed.entries:\n",
        "    news_item = {}\n",
        "    news_item['title'] = entry.title\n",
        "    news_item['category'] = entry.category\n",
        "    soup_description= BeautifulSoup(entry.description, 'html.parser')\n",
        "    news_item['description'] = soup_description.get_text()  \n",
        "    news_item['link'] = entry.link\n",
        "    datetime_rss = parser.parse(entry.published)\n",
        "    datetime_timestamp = float(datetime_rss.strftime(\"%s\"))\n",
        "    local_datetime_converted = datetime.datetime.fromtimestamp(datetime_timestamp)  #μετατροπή ημερομηνίας/ώρας σε μορφή κοινή για όλα τα ειδησεογραφικά κανάλια\n",
        "    news_item['pubDate'] = local_datetime_converted \n",
        "\n",
        "    #  Ανάγνωση του περιεχομένου των ειδήσεων \n",
        "    news = requests.get(entry.link)\n",
        "    news_content = news.content   \n",
        "    soup_news = BeautifulSoup(news_content, 'html.parser')\n",
        "    for script in soup_news(['script']):\n",
        "         script.decompose()    # αφαίρεση ετικετών script και του περιεχομένου τους\n",
        "        \n",
        "       \n",
        "    #ανάγνωση ειδήσεων από το h3 tag στο πάνω  μέρος του άρθρου\n",
        "    try:\n",
        "      div_element = soup_news.find('div', class_='content details')\n",
        "      children = div_element.findChildren('h3' , recursive=False)\n",
        "      h3=children[0].get_text()\n",
        "    except:\n",
        "      h3=\"\"\n",
        "      \n",
        "     \n",
        "    #ανάγνωση του υπόλοιπου άρθρου ειδήσεων\n",
        "    b = soup_news.find_all('div', class_='cntTxt')   \n",
        "    try:\n",
        "      x = b[0].get_text()\n",
        "    except IndexError:\n",
        "      x = \"\"\n",
        "    # Αφαίρεση κενών γραμμών από το περιεχόμενο\n",
        "    lines = x.split(\"\\n\")\n",
        "    non_empty_lines = [line for line in lines if line.strip() != \"\"]\n",
        "    text_without_empty_lines = h3 + \" \"\n",
        "    for line in non_empty_lines:\n",
        "      if line.find(\"Ειδήσεις σήμερα\")!=-1 or line==\"Ακολουθήστε το protothema.gr στο Google News και μάθετε πρώτοι όλες τις ειδήσεις\" :\n",
        "        break\n",
        "      if line.find(\"Glomex Player\")!=-1: \n",
        "        continue  \n",
        "      text_without_empty_lines += line.strip() + \" \"\n",
        "    news_item['content']=text_without_empty_lines.replace('\\n', ' ').strip()\n",
        "    news_items.append(news_item)\n",
        " \n",
        " return news_items\n",
        "\n",
        "\n",
        "# Εξαγωγή ειδήσεων από το ειδησεογραφικό κανάλι tanea.gr \n",
        "def read_tanea_rss():\n",
        "\n",
        " #URL προς απόξεση\n",
        " url_to_scrape=\"https://www.tanea.gr/rss\"\n",
        " NewsFeed = feedparser.parse(url_to_scrape)\n",
        "\n",
        " news_items=[]  \n",
        " for entry in NewsFeed.entries:\n",
        "    news_item = {}\n",
        "    news_item['title'] = entry.title\n",
        "    news_item['category'] = entry.category\n",
        "    soup_description= BeautifulSoup(entry.description, 'html.parser')\n",
        "    news_item['description'] = soup_description.get_text()\n",
        "    news_item['link'] = entry.link\n",
        "    datetime_rss = parser.parse(entry.published)\n",
        "    datetime_timestamp = float(datetime_rss.astimezone(timeZ_At).strftime(\"%s\"))     #μετατροπή ώρας στην τοπική\n",
        "    local_datetime_converted = datetime.datetime.fromtimestamp(datetime_timestamp)   #μετατροπή ημερομηνίας/ώρας σε μορφή κοινή για όλα τα ειδησεογραφικά κανάλια\n",
        "    news_item['pubDate'] = local_datetime_converted \n",
        "    soup_content= BeautifulSoup(entry.content[0]['value'], 'html.parser')             #εξαγωγή του περιεχομένου του άρθρου της είδησης\n",
        "    news_item['content'] =news_item['description'] + soup_content.get_text().replace('\\n', ' ')\n",
        "    news_items.append(news_item)\n",
        " return news_items\n",
        "\n",
        "#επανάληψη της διαδικασίας της εξαγωγής των ειδήσεων κάθε μία ώρα\n",
        "while 1:\n",
        "  #Δημιουργία dataframe df1 από τις ειδήσεις που έχουν εξαχθεί από το κανάλι enikos.gr \n",
        " df = pd.DataFrame(read_enikos_rss(), columns=['title','category', 'description','link','content','pubDate'])\n",
        " try:\n",
        "    df1=df1.append(df) #συνένωση με το dataframe df1 της προηγούμενης επανάληψης\n",
        " except:\n",
        "    df1=df \n",
        " df1=df1.drop_duplicates(subset =\"title\") #αφαίρεση διπλότυπων ειδήσεων\n",
        " \n",
        " #Δημιουργία dataframe df2 από τις ειδήσεις που έχουν εξαχθεί από το κανάλι naftemporiki.gr  \n",
        " df = pd.DataFrame(read_naftemporiki_rss(), columns=['title','category', 'description','link','content','pubDate'])\n",
        " try:\n",
        "    df2=df2.append(df) #συνένωση με το dataframe df2 της προηγούμενης επανάληψης\n",
        " except:\n",
        "    df2=df \n",
        " df2=df2.drop_duplicates(subset =\"title\") #αφαίρεση διπλότυπων ειδήσεων\n",
        " \n",
        " #Δημιουργία dataframe df3 από τις ειδήσεις που έχουν εξαχθεί από το κανάλι in.gr  \n",
        " df = pd.DataFrame(read_in_rss(), columns=['title','category', 'description','link','content','pubDate'])\n",
        " try:\n",
        "    df3=df3.append(df)  #συνένωση με το dataframe df3 της προηγούμενης επανάληψης\n",
        " except:\n",
        "    df3=df \n",
        " df3=df3.drop_duplicates(subset =\"title\")  #αφαίρεση διπλότυπων ειδήσεων\n",
        " \n",
        " #Δημιουργία dataframe df4 από τις ειδήσεις που έχουν εξαχθεί από το κανάλι protothema.gr  \n",
        " df = pd.DataFrame(read_protothema_rss(), columns=['title','category', 'description','link','content','pubDate'])\n",
        " try:\n",
        "    df4=df4.append(df)  #συνένωση με το dataframe df4 της προηγούμενης επανάληψης\n",
        " except:\n",
        "    df4=df \n",
        " df4=df4.drop_duplicates(subset =\"title\") #αφαίρεση διπλότυπων ειδήσεων\n",
        " \n",
        " #Δημιουργία dataframe df5 από τις ειδήσεις που έχουν εξαχθεί από το κανάλι tanea.gr  \n",
        " df = pd.DataFrame(read_tanea_rss(), columns=['title','category', 'description','link','content','pubDate'])\n",
        " try:\n",
        "    df5=df5.append(df)  #συνένωση με το dataframe df5 της προηγούμενης επανάληψης\n",
        " except:\n",
        "    df5=df \n",
        " df5=df5.drop_duplicates(subset =\"title\")  #αφαίρεση διπλότυπων ειδήσεων \n",
        "  \n",
        " #συνένωση των dataframes df1, df2, df3, df4, df5\n",
        " frames = [df1, df2, df3,df4,df5]\n",
        " df = pd.concat(frames)\n",
        " df.reset_index(drop=True, inplace=True)\n",
        "\n",
        " display(df)\n",
        " time.sleep(60*60) #επανάληψη της διαδικασίας εξαγωγής των ειδήσεων κάθε μία ώρα"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRXVRLNxtIcd"
      },
      "source": [
        "# Προεπεξεργασία ειδήσεων"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lngJA3ytXOU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import unicodedata as ud\n",
        "!pip install greek-stemmer\n",
        "from greek_stemmer import GreekStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R4ullbhztk-e"
      },
      "outputs": [],
      "source": [
        "stemmer = GreekStemmer()  # stemmer για την ελληνική γλώσσα\n",
        "\n",
        "stopWordsGreek = stopwords.words('greek')      # Λήψη της λίστας με τις ελληνικές στοπ λέξεις \n",
        "stopWordsEnglish = stopwords.words('english')  # Λήψη της λίστας με τις αγγλικές στοπ λέξεις \n",
        "stopWords=stopWordsGreek + stopWordsEnglish\n",
        "\n",
        "# Προσθήκη νέων stop λέξεων στη λίστα\n",
        "stopWords.extend(['ακόμη','ακόμα','ακριβώς','αλλά','άλλος', 'αλλού','άλλα','άλλη','άλλο','άλλοι','άλλων','άλλους','άλλες','άμεσα','αμέσως','αν','ανά','ανάμεσα','αναφέρει','ανέφερε','αντ', 'αντί','αντίθετα','αντίστοιχα','άνω', 'από','απ','άρα','αργά','αργότερα','αρκετά','αρκετός','αρκετή','αρκετοί','αρκετές','αρκετό','αρκετών','αρκετούς','αρχικά','αρχικό','αρχική','αρχικός','αρχικοί','αρχικές','αρχικούς','αρχικών','ας','αφορά','αφού','αφότου','αύριο', 'αυτά', 'αυτές', 'αυτή','αυτήν', 'αυτό', 'αυτοί', 'αυτός', 'αυτούς', 'αυτών','αυτόν','αυτής','αυτού','βέβαια','γενικά','γενικώς','γρ','για','γιατί','γι','γίνομαι','γίνεται','γρήγορα','γύρω','διαβάστε','δηλαδή','δήλωσε','δε','δει','δείτε','δειτε','δεν','δικό','δικός','δική','δικοί','δικούς','δικών','διότι','δίπλα','δύο','δυο',\n",
        "                   'εάν','εαυτόν','εαυτό','έγκαιρα','εγκαίρως','εδώ','εδω','είδα','είδες','είδε','είδαμε','είδατε','είδαν','είδανε','ειδικά','είπα','είπες','είπε','είπαμε','είπατε','είπαν','εκ','έκανε','έκαναν','έγινε','έγιναν','εν','εντός','έναντι','ένα','έναν','ένας','ενός','ενόψει','επί','επι','επιπλέον','εκεί', 'εγώ','εσύ','είμαι', 'είμαστε', 'είναι', 'είσαι', 'είστε', 'εκείνα', 'εκείνες', 'εκείνη','εκείνο', 'εκείνος', 'εκείνοι', 'εκείνους', 'εκείνων','εμάς','εσάς','εμείς','εσείς','εξάλλου','εξής','έξω','έπειτα','επίσης','επιπλέον','επειδή','επομένως','έπρεπε', 'ενώ','εμένα','εσένα','εσυ','έτσι', 'έως','είπε', 'είπαμε','είπατε','είπαν',\n",
        "                   'είτε','εκτός','εμπρός','εντελώς','έχω','έχεις','έχει','έχουμε','έχετε','έχουν','είχε','είχαν','είχα','είχαμε','είχατε','είχες','ευρέως','εφόσον','ήμουν','ήσουν','ήταν','η','ή', 'ήδη','θα','ίδια','ίδιος','ίδιο','ίδιες','ίδιοι', 'ιδίως','ίσα','ίσια','ίσως', 'και', 'κάνω','κάνεις','κάνει','κάνουμε','κάνετε','κάνουν','κάθε', 'καθένας','καθεμια','καθεμία','καθόλου','καθώς','καλά','κακά', 'καμία','καμια','κατά','κι','καν','κανείς','κανένας','κανένα','κάποιος','κάποια','κάποιο','κάποιοι','κάποιες','κάποτε','κάτι','κάτω','κιόλας','κοντά','κυρίως','λέει','λένε','λιγάκι','λίγο','λίγες','λίγων','λίγοι','λίγα','λίγος','λίγης','λίγους','λιγότερο',\n",
        "                   'λιγότερα','λιγότερος','λιγότεροι','λιγότερη','λιγότερες','λιγότερων','λοιπόν','λόγω','μαζί','μάλιστα','μάλλον','μέσα','μέσω','μεταξύ','μετά','μέχρι','μη','μήπως','μίλησε','μόλις','μας','μένα' ,'με','μερικοί','μερικών','μερικούς','μερικές','μερικά','μολονότι','μην','μόνο','μόνον','μόνος','μόνη','μου','μια','μία','μιας','μίας','μπορώ','μπορείς','μπορεί','μπορούμε','μπορείτε','μπορούν','νά','να','ναι','νομίζω','νομίζεις','νομίζει','νομίζουμε','νομίζετε','νομίζουν','νωρίς','νωρίτερα','ξανά', 'ο', 'οι','όλο','όλος','όλη','όλοι','όλες','όλα','όλους','όλων','όμως','όντως','όπου','οποία','οποίο','οποίος','οποίων','οποίους','οποίοι','οποίου','οποίες',\n",
        "                   'οπότε','όπως','ορισμένο','ορισμένα','ορισμένοι','ορισμένες','όσο','όσος','όση','όσα','όσες','όσοι','όσων','όσους','όσον','όποιος','όποιο','όποια','όποιο','όποιων','όποιοι','όποιους','όποιες','οποιοσδήποτε','οποιαδήποτε','οποιοδήποτε','οποιουδήποτε','οποιασδήποτε','οποιοιδήποτε','οποιεσδήποτε','οποιωνδήποτε','οποιουσδήποτε','ος', 'ότι', 'ό,τι','όταν','ούτε','όχι','πάντα','πάλι','πάνω','πάρα','παρακολουθήστε','παρόλα','παρόλο','πει','πέρα','περίπου','περισσότερα','πια','πίσω','πιθανό','πιθανόν','ποια','ποιες', 'ποιο', 'ποιοι','ποιόν','ποιος','ποιον', 'πότε','ποτέ','πιο','πλέον','ποιους', 'ποιων','πόσο','πόσος','πόση','πόσα','πόσοι','πόσες','που','πού',\n",
        "                   'πολύ','πολλοί','πολλά','πολλών','πολλούς','πολλές','πράγματι','πρέπει','πριν','πρόκειται','προτού','πως','πώς','σαν','σας','σε','σου','σιγά','σήμερα', 'στη', 'στην', 'στο', 'στον', 'στα','στιγμή','στις','στους','συ','σύμφωνα','συνεπώς','συνέχεια','συνεχώς','συνήθως','συχνά','σχεδόν','σχετικός','σχετική','σχετικό','σωστά','τα','ταυτόχρονα','ταυτόχρονη','τελικά','το', 'την', 'τη','τι', 'τις','της','των','τέτοιο','τέτοια','τέτοιος','τέτοιοι','τέτοιων','τέτοιους','τέτοιες','τέτοιον','τίποτα','τόνισε','τόνισαν','τουλάχιστον','τόσο','τόσα','τόσος','τόση','τόσοι','τόσων','τόσους','τόσες','τούτος','τούτη','τούτο','τούτα','τούτες','τον',\n",
        "                   'τότε', 'του', 'τους','τώρα','υπάρχει','υπάρξει','υπάρχουν','υπό','ύστερα','ως','ώσπου','ώστε','ωστόσο','προς','μία','φαίνεται','φορά','φυσικά','χαμηλά','χάρη','χθες','χωρίς', 'proto', 'thema','Πηγή:www.gazzetta.gr','Πηγή: ΑΠΕ-ΜΠΕ','marieclaire','gr','www','gazzetta','com','πηγή','απε','μπε','pic','twitter','live','view','post','instagram','shared','ygeiamou','epiruspostgr','https'])\n",
        "\n",
        "d = {ord('\\N{COMBINING ACUTE ACCENT}'):None} # για την αφαίρεση τόνων από τις ελληνικές λεξεις"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dS4XYKCax0a9"
      },
      "outputs": [],
      "source": [
        "# συνάρτηση για δημιουργία tokens και καθαρισμός του κειμένου\n",
        "def tokenize_only(str_input):\n",
        "    bagOfWords = nltk.word_tokenize(str_input) # δημιουργία tokens\n",
        "    \n",
        "    clean_tokens=[]\n",
        "   \n",
        "    for token in bagOfWords:\n",
        "      token = token.lower()\n",
        "      # αφαίρεση κάθε τιμής που δεν ανήκει στο ελληνικό ή αγγλικό αλφάβητο\n",
        "      new_token = re.sub(r'[^α-ωΑ-Ωά-ώΆ-ΏΆ-Ώa-zA-Z]+', '', token) \n",
        "      # αφαίρεση των tokens μεγέθους ενός  χαράακτήρα\n",
        "      if  len(new_token.strip()) >= 2: \n",
        "         vowels=len([v for v in new_token if v in \"αάεέιίηήοόυύωώaeiouy\"])\n",
        "         if vowels != 0 and new_token not in stopWords: # αφαίρεση των λέξεων που περιέχουν μόνο σύμφωνα και δεν ανήκουν στη λίστα των stop λέξεων\n",
        "             clean_tokens.append(new_token)     \n",
        "          \n",
        "    return clean_tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Pt2K2L0dZIqU"
      },
      "outputs": [],
      "source": [
        "# συνάρτηση για δημιουργία tokens,καθαρισμός του κειμένου και stemming\n",
        "def tokenize_and_stem(str_input):\n",
        "    bagOfWords = nltk.word_tokenize(str_input) # δημιουργία tokens\n",
        "    \n",
        "    stem_sentence=[]\n",
        "    for token in bagOfWords:\n",
        "      token = token.lower()\n",
        "      # αφαίρεση κάθε τιμής που δεν ανήκει στο ελληνικό ή αγγλικό αλφάβητο\n",
        "      new_token = re.sub(r'[^α-ωΑ-Ωά-ώΆ-ΏΆ-Ώa-zA-Z]+', '', token) \n",
        "      # αφαίρεση των tokens μεγέθους ενός  χαράακτήρα\n",
        "      if  len(new_token.strip()) >= 2: \n",
        "         vowels=len([v for v in new_token if v in \"αάεέιίηήοόυύωώaeiouy\"])\n",
        "         if vowels != 0 and new_token not in stopWords:  # αφαίρεση των λέξεων που περιέχουν μόνο σύμφωνα και δεν ανήκουν στη λίστα των stop λέξεων\n",
        "               stem_sentence.append(stemmer.stem(ud.normalize('NFD',new_token.upper()).translate(d)))  # μετατροπή σε κεφαλαία, αφαίρεση τόνων και stemming των λέξεων\n",
        "  \n",
        "    return stem_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_IrUAETtwKC"
      },
      "outputs": [],
      "source": [
        "# δημιουργία dataframe vocab_frame όπου κάθε γραμμή περιέχει την λέξη που έχει υποστεί stemming και την λέξη χωρίς stemming\n",
        "totalvocab_stemmed = []     # λίστα με όλα τα tokens που έχουν υποστεί stemming\n",
        "totalvocab_tokenized = []   # λίστα με όλα τα tokens\n",
        "for i in df['content'].tolist():\n",
        "    allwords_stemmed = tokenize_and_stem(i) # για κάθε είδηση του dataframe, tokenize και stemming\n",
        "    totalvocab_stemmed.extend(allwords_stemmed) # επέκταση της 'totalvocab_stemmed' λίστας\n",
        "    \n",
        "    allwords_tokenized = tokenize_only(i)  # για κάθε είδηση του dataframe, tokenize\n",
        "    totalvocab_tokenized.extend(allwords_tokenized)# επέκταση της 'totalvocab_tokenized' λίστας\n",
        "\n",
        "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed) # δημιουργία του dataframe vocab_frame\n",
        "display(vocab_frame.head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPXec1sjuOy1"
      },
      "source": [
        "# Ομαδοποίηση ειδήσεων με k-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eLimhIYCuY53"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqRCqoEg-3_g"
      },
      "source": [
        "Μετατροπή της συλλογής των ειδήσεων σε έναν tf-idf matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxmioCZW-uqa"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopWords, tokenizer=tokenize_and_stem,\n",
        "                       analyzer='word',lowercase=True)\n",
        "                      \n",
        "dtm_tfidf = tfidf_vectorizer.fit_transform(df['content'].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nplpwra9_LVo"
      },
      "source": [
        "Εφαρμογή αλγορίθμου k-means για ομαδοποίηση ειδήσεων"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHULCg1M_msR"
      },
      "outputs": [],
      "source": [
        "num_clusters=int(input('Give the number of clusters: ')) # Πλήθος των ομάδων ειδήσεων που θα δημιουργηθούν\n",
        "# Έκτέλεση του αλγορίθμου k-means\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(dtm_tfidf)\n",
        " \n",
        "clusters = km.labels_.tolist()\n",
        "news = { 'title': df['title'].tolist(), 'category': df['category'].tolist(), 'content': df['content'].tolist(), 'cluster': clusters }\n",
        "frame = pd.DataFrame(news,  columns = ['cluster', 'category', 'title', 'content']) # Δημιουργία dataframe με στήλες:Cluster, κατηγορία, τίτλος, περιεχόμενο \n",
        "print(\"Number of of news articles per cluster\")\n",
        "print(frame['cluster'].value_counts()) # εμφάνιση πλήθους ειδήσεων ανά cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMVubs4Y_rzT"
      },
      "source": [
        "Στατιστικά και γραφήματα από την ομαδοποίηση ειδήσεων"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2grTSkGq2FGU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "prediction = km.predict(dtm_tfidf) # προβλέπει την πλησιέστερη ομάδα που ανήκει κάθε μια από τις ειδήσεις\n",
        "# συνάρτηση για στατιστικά και γραφήματα από την ομαδοποίηση ειδήσεων\n",
        "def get_top_words_cluster(tf_idf_array, prediction, n_w):\n",
        "    labels = np.unique(prediction)\n",
        "        \n",
        "    dfs = []\n",
        "    for label in labels:\n",
        "        id_temp = np.where(prediction==label) # indexes  για κάθε cluster\n",
        "        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # επιστρέφει τη μέση βαθμολογία στο cluster\n",
        "        sorted_means = np.argsort(x_means)[::-1][:n_w] # indexes με τις top 15 βαθμολογίες\n",
        "        features = tfidf_vectorizer.get_feature_names_out()\n",
        "        top_words=[]\n",
        "        # εμφάνιση ολόκληρων των λέξεων και όχι των αποκομμένων\n",
        "        for i in sorted_means:\n",
        "           if len(vocab_frame.loc[features[i],'words'][0])==1:\n",
        "             t=(vocab_frame.loc[features[i],'words'], x_means[i])\n",
        "           else:\n",
        "             t=(vocab_frame.loc[features[i],'words'][0], x_means[i])\n",
        "           top_words.append(t) \n",
        "        df = pd.DataFrame(top_words, columns = ['words', 'score']) # δημιουργία dataframe με στήλες τις λέξεις και τη βαθμολογία\n",
        "        df.plot.barh(x ='words', y='score', color=['red','blue'])\t# γραφική αναπαράσταση του dataframe\n",
        "        plt.title(label)\n",
        "        plt.show()\n",
        "        print(df)\n",
        "        dfs.append(df)\n",
        "get_top_words_cluster(dtm_tfidf.toarray(), prediction, 15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYBidu3lutm6"
      },
      "source": [
        "# LDA Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMFL07qVu0HD"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "!pip install pyldavis==2.1.2\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oztzQQ76AEyv"
      },
      "source": [
        "Μετατροπή της συλλογής των ειδήσεων σε έναν tf matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_VP9qM4kAI2i"
      },
      "outputs": [],
      "source": [
        "tf_vectorizer = CountVectorizer(analyzer='word', stop_words=stopWords, \n",
        "                             lowercase=True,tokenizer=tokenize_and_stem)\n",
        "dtm_tf = tf_vectorizer.fit_transform(df['content'].tolist())\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpRgsZByATKT"
      },
      "source": [
        "Δημιουργία lda μοντέλου και οπτικοποίηση των topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8_O_tyKrFe9"
      },
      "outputs": [],
      "source": [
        "# topic modelling - 20 topics\n",
        "lda_tf = LatentDirichletAllocation(n_components=20,max_iter=20, random_state=20)\n",
        "lda_tf.fit(dtm_tf)\n",
        "# οπτικοποίηση των topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis=pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer) \n",
        "display(pyLDAvis.display(vis))\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZsDf13bvDbd"
      },
      "source": [
        "# Faiss search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF6_dZenHtil"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-gpu\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SBF8BB2HvF5"
      },
      "outputs": [],
      "source": [
        "# λήψη του μοντέλου 'sn-xlm-roberta-base-snli-mnli-anli-xnli' και δημιουργία embeddings\n",
        "model = SentenceTransformer('symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli')\n",
        "data=df.content.to_list()\n",
        "encoded_data = model.encode(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az8hqwUGH8ol"
      },
      "outputs": [],
      "source": [
        "# Αποθήκευση των sentence embeddings σε έναν faiss index\n",
        "index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
        "index.add_with_ids(encoded_data, np.array(range(0, len(data))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADCjxapQISug"
      },
      "outputs": [],
      "source": [
        "#Αλλαγή από cpu σε gpu\n",
        "res = faiss.StandardGpuResources()\n",
        "gpu_index = faiss.index_cpu_to_gpu(res, 0, index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_ogCQr1hfj6"
      },
      "source": [
        "# Εγκατάσταση ElasticSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZPfNGjPiNf2"
      },
      "source": [
        "Εγκατάσταση elasticsearch server version 7.12.0. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyxev65XiPzh"
      },
      "outputs": [],
      "source": [
        "!apt install default-jdk > /dev/null\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.12.0-linux-x86_64.tar.gz -q --show-progress\n",
        "!tar -xzf elasticsearch-7.12.0-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.12.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_DvvBMPigG9"
      },
      "source": [
        "\n",
        "Εκκίνηση του Elasticsearch Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpZuYAaziqTt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.12.0/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_iKmwVnixMp"
      },
      "outputs": [],
      "source": [
        "# Αναμονή για λίγα δευτερόλεπτα μέχρι να ξεκινήσει ο server.\n",
        "import time\n",
        "time.sleep(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA_wy-7Fi7CE"
      },
      "source": [
        "Εγκατάσταση του Elasticsearch client και έλεγχος αν ο client μπορεί να έχει πρόσβαση στον Elasticsearch server.Εκτέλεση δύο φορές αν χρειάζεται, ώστε να δώσουμε χρόνο στον server να ξεκινήσει."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TjR4SS0i83g"
      },
      "outputs": [],
      "source": [
        "!pip install elasticsearch==7.12.0 -q\n",
        "from elasticsearch import Elasticsearch\n",
        "es = Elasticsearch(\"http://localhost:9200\")\n",
        "es.ping()  # πρέπει να λάβουμε True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtE9L93pjG6N"
      },
      "source": [
        "Έλεγχος αν το elasticsearch δουλεύει κανονικά "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-33XNYgjLKa"
      },
      "outputs": [],
      "source": [
        "!curl -X GET \"localhost:9200/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY2Ygw0gpNiN"
      },
      "source": [
        "# Προσθήκη των ειδήσεων στον ElasticSearch index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C3MZjaupRVN"
      },
      "outputs": [],
      "source": [
        "# Σύνδεση στο elasticsearch \n",
        "es = Elasticsearch(\"http://localhost:9200\")\n",
        "#διαγραφή του  index αν υπάρχει ήδη\n",
        "es.indices.delete(index='news_index', ignore=[400, 404])\n",
        "# δημιουργία του index ανα δεν υπάρχει\n",
        "try:\n",
        "    request_body = {\n",
        "        \"settings\" : {\n",
        "            \"number_of_shards\": 1,\n",
        "            \"number_of_replicas\": 1\n",
        "        },\n",
        "\n",
        "        \"mappings\": {\n",
        "            \"properties\": {\n",
        "                \"title\": {\"type\": \"text\"},\n",
        "                \"category\": {\"type\": \"text\"},\n",
        "                \"description\": {\"type\": \"text\"},\n",
        "                \"link\": {\"type\":  \"text\"},\n",
        "                \"pubDate\": {\"type\": \"text\"},\n",
        "                \"body\": {\"type\": \"text\"},\n",
        "                \"body_vector\": {\"type\":  \"dense_vector\", \"dims\":  768}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    es.indices.create(index = 'news_index', body = request_body)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Προσθήκη κάθε άρθρου είδήσεων στο index. \n",
        "for i in range(len(df)):\n",
        "            news = {\n",
        "                'title': df['title'][i],\n",
        "                'category':df['category'][i],\n",
        "                'description': df['description'][i],\n",
        "                'link': df['link'][i],\n",
        "                'body': df['content'][i],\n",
        "                'body_vector': model.encode(df['content'][i], convert_to_tensor=True).tolist(),\n",
        "                'pubDate': df['pubDate'][i]\n",
        "                }\n",
        "            res = es.index(index=\"news_index\", id=i, body=news)\n",
        "            \n",
        "time.sleep(1)\n",
        "res=es.get(index='news_index',id=1)\n",
        "print(res)\n",
        "print('Process complete')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oeDvuBIwWWj"
      },
      "outputs": [],
      "source": [
        "!curl -X GET \"localhost:9200/_cat/indices?v\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWG7MhzD6qIW"
      },
      "source": [
        "#Συνάρτηση για την εύρεση των ειδήσεων"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSTtofIKwycb"
      },
      "outputs": [],
      "source": [
        "def find_news(user_input, search):\n",
        "  if search==\"elsearch\": # αν έχει επιλεγεί αναζήτηση με elasticsearch\n",
        "    es = Elasticsearch()\n",
        "    SEARCH_SIZE = 10 # Μέγιστος αριθμός αποτελεσμάτων που θα επιστραφούν\n",
        "    query_vector = model.encode(user_input, convert_to_tensor=True).tolist() # δημιουργία embeddings από το ερώτημα που υποβάλλεται\n",
        "    print(query_vector[0:10])\n",
        "    # χρήση του ερωτήματος\n",
        "    script_query = {\n",
        "            \"script_score\": {\n",
        "                \"query\": {\"match_all\": {}},\n",
        "                \"script\": {\n",
        "                    \"source\": \"cosineSimilarity(params.query_vector, 'body_vector') + 1.0\",\n",
        "                    \"params\": {\"query_vector\": query_vector}\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # αναζήτηση στο index\n",
        "    res = es.search(\n",
        "        index='news_index',  # το όνομα του index\n",
        "        body={\n",
        "            \"size\": SEARCH_SIZE,\n",
        "            \"query\": script_query,\n",
        "            \"_source\": {\"includes\": [\"title\", \"body\", \"body_vector\", \"link\"]}\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # δημιουργία λίστας με τις ειδήσεις που βρέθηκαν\n",
        "    news = []\n",
        "    header='<h2>Matches to query: {input} </h2>'\n",
        "    news.append(header.format(input=user_input))\n",
        "    base_element = '<a href=\"{url}\">{title}</a> - <b>News</b>: <i>{body}</i>'\n",
        "    for hit in res['hits']['hits']:\n",
        "        news.append(base_element.format(url=hit['_source']['link'], title=hit['_source']['title'],body=hit['_source']['body']))\n",
        "    return news\n",
        "  else:  #αν έχει επιλεγεί αναζήτηση με faiss\n",
        "    t=time.time()\n",
        "    query_vector = model.encode([user_input])\n",
        "    k = 10\n",
        "    top_k = index.search(query_vector, k)\n",
        "    print(top_k)\n",
        "    results=top_k[1].tolist()[0]\n",
        "   \n",
        "    print('totaltime: {}'.format(time.time()-t))\n",
        "    # δημιουργία λίστας με τις ειδήσεις που βρέθηκαν\n",
        "    news = []\n",
        "    header='<h2>Matches to query: {input} </h2>'\n",
        "    news.append(header.format(input=user_input))\n",
        "    base_element =  '<a href=\"{url}\">{title}</a> - <b>News</b>: <i>{body}</i>'\n",
        "    for _id in top_k[1].tolist()[0]:\n",
        "        news.append(base_element.format(url=df['link'][_id], title=df['title'][_id],body=df['content'][_id]))   \n",
        "    \n",
        "    return news\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Tlx1p65g-H"
      },
      "source": [
        "# Εκκίνηση της διεπαφής χρήστη"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dDpDGiErXaP"
      },
      "outputs": [],
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install flask==0.12.2  # Νεότερες εκδόσεις του flask δεν δουλεύουν στο  Colab\n",
        "                            # Βλέπε https://github.com/plotly/dash/issues/257\n",
        "\n",
        "!pip install pyngrok\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaY02xOxrYRy"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok authtoken 22f2I1Y6Jkb0FSVz8J6kgWu9GPc_y3BHnWUbv3CA8NAWQy9L # αποθήκευση του κλειδιού Authtoken στο αρχείο ρυθμίσεων ngrok.yml \n",
        "from flask import Flask, request, render_template\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from google.colab import drive\n",
        "# στο drive υπάρχει ο φάκελος templates με τα αρχεια index.htlm και results.html\n",
        "drive.mount('/content/drive')\n",
        "app = Flask(__name__,template_folder='/content/drive/My Drive/templates') \n",
        "\n",
        "run_with_ngrok(app)\n",
        "@app.after_request\n",
        "def after_request(response):\n",
        "    response.headers[\"Cache-Control\"] = \"no-cache, no-store, must-revalidate, public, max-age=0\"\n",
        "    response.headers[\"Expires\"] = '0'\n",
        "    response.headers[\"Pragma\"] = \"no-cache\"\n",
        "    return response\n",
        "\n",
        "# σελίδα Index\n",
        "@app.route('/', methods=['get', 'post'])            \n",
        "\n",
        "def index_page():\n",
        "    return render_template('index.html')\n",
        "\n",
        "# Λήψη δεδομένων από τη φόρμα στη σελίδα index, και πέρασμα αυτών στη συνάρτηση find_news()\n",
        "@app.route('/results', methods=['get', 'post'])\n",
        "def results_page():\n",
        "    if request.method == 'POST':\n",
        "        user_input = request.form[\"user_input\"]\n",
        "        print(user_input)\n",
        "        check = request.form[\"search\"]\n",
        "        if check==\"elsearch\": # αν έχει επιλεγεί αναζήτηση με elasticsearch\n",
        "           search=\"elsearch\" \n",
        "        else:\n",
        "           search=\"faiss\"  # αν έχει επιλεγεί αναζήτηση με faiss\n",
        "               \n",
        "        news = find_news(user_input,search)       \n",
        "        return render_template('results.html',data=news)  # επιστροφή της σελίδας results.html με τα αποτελέσματα αναζήτησης\n",
        "    else:\n",
        "        return \"An error was encountered\"\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KRXVRLNxtIcd",
        "oPXec1sjuOy1",
        "LYBidu3lutm6"
      ],
      "name": "search_news.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
